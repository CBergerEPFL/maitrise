{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import desolver.backend as D\n",
    "from math import isnan\n",
    "from numba import njit,prange\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "path_formatted_glasgow = \"/workspaces/maitrise/data/20221006_physio_quality/set-a/dataParquet\"\n",
    "path_petastorm = f\"file:///{path_formatted_glasgow}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's re write each function in a numba compatible version:\n",
    "\n",
    "@njit\n",
    "def taux_Mean_fast(signal, taux, hprime, h=0):\n",
    "    return np.mean((signal[int(h + taux) : int(taux + hprime + h)]))\n",
    "\n",
    "@njit\n",
    "def taux_var_fast(signal, taux, hprime, h=0):\n",
    "    return np.var(signal[int(h + taux) : int(taux + hprime + h)])\n",
    "\n",
    "@njit(parallel = True)\n",
    "\n",
    "def adapted_c(c_val,fs,h,hprime,signal):\n",
    "    for l in c_val:\n",
    "        if (\n",
    "            l * fs + hprime * len(signal) + h * len(signal) > len(signal)\n",
    "            and l * fs + h * len(signal) > len(signal) - 1\n",
    "        ):\n",
    "            c = c_val[c_val < l]\n",
    "            break\n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:402: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:362: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if not dataset.common_metadata:\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:368: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  dataset_metadata_dict = dataset.common_metadata.metadata\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/reader.py:418: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  self.dataset = pq.ParquetDataset(dataset_path, filesystem=pyarrow_filesystem,\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:253: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  metadata = dataset.metadata\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:254: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  common_metadata = dataset.common_metadata\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:278: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  sorted_pieces = sorted(dataset.pieces, key=attrgetter('path'))\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:288: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/etl/dataset_metadata.py:288: FutureWarning: ParquetDatasetPiece is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/py_dict_reader_worker.py:146: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/py_dict_reader_worker.py:182: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partitions = self._dataset.partitions\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/py_dict_reader_worker.py:267: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  data_frame = piece.read(columns=column_names, partitions=self._dataset.partitions).to_pandas(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/py_dict_reader_worker.py:146: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/py_dict_reader_worker.py:182: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partitions = self._dataset.partitions\n",
      "/workspaces/maitrise/__pypackages__/3.10/lib/petastorm/py_dict_reader_worker.py:267: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  data_frame = piece.read(columns=column_names, partitions=self._dataset.partitions).to_pandas(\n"
     ]
    }
   ],
   "source": [
    "##signal trial\n",
    "\n",
    "with make_reader(path_petastorm) as reader:\n",
    "    for sample in reader:\n",
    "        data = sample\n",
    "        if data.signal_quality == \"acceptable\".encode():\n",
    "            break\n",
    "        else : \n",
    "            pass\n",
    "\n",
    "ECG_signal = data.signal\n",
    "ECG_lead = data.signal_names\n",
    "fs = data.sampling_frequency\n",
    "\n",
    "dico_ECG = {}\n",
    "\n",
    "for i,j in zip(ECG_lead,range(12)):\n",
    "     dico_ECG[i] = ECG_signal[:,j]\n",
    "\n",
    "print(len(dico_ECG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000e+00 2.000e-03 4.000e-03 ... 9.984e+00 9.986e+00 9.988e+00]\n"
     ]
    }
   ],
   "source": [
    "###Let's give them a try\n",
    "\n",
    "h = 0.001\n",
    "hprime = 0.005\n",
    "sig = dico_ECG[ECG_lead[0]]\n",
    "c = np.arange(0, (len(sig) / fs) + 0, 1 / fs,dtype = np.float64)\n",
    "c_t = adapted_c(c,fs,h,hprime,sig.copy())\n",
    "print(c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now for the big shot:\n",
    "@njit\n",
    "def I1(c, signal, fs, h, hprime, step_c, t0=0):\n",
    "    tab = np.zeros_like(c,dtype = np.float64)\n",
    "    for count in prange(len(tab)):\n",
    "        if count ==0:\n",
    "            I1c = (\n",
    "                (1 / (h * len(signal)))\n",
    "                * step_c\n",
    "                * np.abs(\n",
    "                    taux_Mean_fast(signal, t0 * fs, hprime * len(signal), h * len(signal))\n",
    "                    - taux_Mean_fast(signal, t0 * fs, hprime * len(signal))\n",
    "                )\n",
    "            )\n",
    "            tab[count] = I1c\n",
    "        else : \n",
    "            I1c = tab[count-1]\n",
    "            I1c = I1c + (\n",
    "                (1 / (h * len(signal)))\n",
    "                * step_c\n",
    "                * np.abs(\n",
    "                    taux_Mean_fast(signal, t0 * fs, hprime * len(signal), h * len(signal))\n",
    "                    - taux_Mean_fast(signal, t0 * fs, hprime * len(signal))\n",
    "                )\n",
    "            )\n",
    "            tab[count-1] = I1c\n",
    "    return tab[:-1]\n",
    "\n",
    "@njit\n",
    "def I2(c, signal, fs, h, hprime, step_c, t0=0):\n",
    "    tab = np.zeros_like(c,dtype = np.float64)\n",
    "    for count in prange(len(tab)):\n",
    "        if count ==0:\n",
    "            I1c = (\n",
    "                (1 / (h * len(signal)))\n",
    "                * step_c\n",
    "                * np.abs(\n",
    "                    taux_var_fast(signal, t0 * fs, hprime * len(signal), h * len(signal))\n",
    "                    - taux_var_fast(signal, t0 * fs, hprime * len(signal))\n",
    "                )\n",
    "            )\n",
    "            tab[count] = I1c\n",
    "        else : \n",
    "            I1c = tab[count-1]\n",
    "            I1c = I1c + (\n",
    "                (1 / (h * len(signal)))\n",
    "                * step_c\n",
    "                * np.abs(\n",
    "                    taux_var_fast(signal, t0 * fs, hprime * len(signal), h * len(signal))\n",
    "                    - taux_var_fast(signal, t0 * fs, hprime * len(signal))\n",
    "                )\n",
    "            )\n",
    "            tab[count-1] = I1c\n",
    "    return tab[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.42399716e-09 3.71199858e-09 3.71199858e-09 ... 3.71199858e-09\n",
      " 3.71199858e-09 3.71199858e-09]\n"
     ]
    }
   ],
   "source": [
    "##Trial : \n",
    "\n",
    "I1_t = I2(c_t,sig,fs,h,hprime,1/fs)\n",
    "print(I1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now let's create the main runner : \n",
    "@njit\n",
    "def discrepancies_mean_curve(signal_tot, fs, h, hprime, step, t0=0):\n",
    "    c1 = np.arange(t0, (len(signal_tot) / fs) + t0, step,dtype = np.float64)\n",
    "    c_adapted = adapted_c(c1,fs,h,hprime,signal_tot)\n",
    "    I1_t = I1(c_adapted, signal_tot, fs, h, hprime, step, t0)\n",
    "    I2_t = I2(c_adapted, signal_tot, fs, h, hprime, step, t0)\n",
    "    return I1_t, I2_t, c_adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test : \n",
    "\n",
    "I1_test,I2_test,c = discrepancies_mean_curve(sig,fs,h,hprime,1/fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Der interval calculator for each lead \n",
    "@njit\n",
    "def Interval_calculator_lead_fast(signal, fs, t0=0):\n",
    "    h = 0.001\n",
    "    hprime = 0.005\n",
    "    c1 = np.arange(t0, (len(signal) / fs) + t0, 1/fs)\n",
    "    c_adapted = adapted_c(c1,fs,h,hprime,signal)\n",
    "    I1c = I1(c_adapted, signal, fs, h, hprime, 1/fs, t0)\n",
    "    I2c = I2(c_adapted, signal, fs, h, hprime, 1/fs, t0)\n",
    "    c1 = c[I1c[~np.isnan(I1c)]==np.max(I1c[~np.isnan(I1c)]) / 2]\n",
    "    c2 = c[I2c[~np.isnan(I2c)]==np.max(I2c[~np.isnan(I2c)]) / 2]\n",
    "    cs = np.minimum(np.mean(c1), np.mean(c2))\n",
    "    dic_segment_lead = (cs - t0) * fs\n",
    "    if isnan(dic_segment_lead) or dic_segment_lead<100 :\n",
    "        dic_segment_lead = 2500\n",
    "    return dic_segment_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496.9999999999995\n"
     ]
    }
   ],
   "source": [
    "##Test : \n",
    "optimal_test = Interval_calculator_lead_fast(sig,fs)\n",
    "print(optimal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now for all The lead :\n",
    "\n",
    "def Interval_calculator_all(dico_signal, name_signal, fs):\n",
    "    dic_segment_lead = {}\n",
    "    for i in name_signal:\n",
    "        dic_segment_lead[i] = int(Interval_calculator_lead_fast(dico_signal[i], fs))\n",
    "    return dic_segment_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'I': 2496, b'II': 2496, b'III': 2496, b'aVR': 2496, b'aVF': 2496, b'aVL': 2496, b'V1': 2496, b'V2': 2496, b'V3': 2496, b'V4': 2496, b'V5': 2496, b'V6': 2496}\n"
     ]
    }
   ],
   "source": [
    "##Test : \n",
    "dic_segment_lead = Interval_calculator_all(dico_ECG,ECG_lead,fs)\n",
    "print(dic_segment_lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now the element of the TSD:\n",
    "\n",
    "from numpy import int64\n",
    "\n",
    "\n",
    "@njit\n",
    "def Lm_q(signal1, m, k, fs):\n",
    "    N = len(signal1)\n",
    "    n = np.floor((N - m) / k)\n",
    "    norm = (N - 1) / (n * k * (1 / fs))\n",
    "    #sum = np.sum(np.abs(np.diff(signal1[m::k], n=1)))\n",
    "    sum = 0\n",
    "    for i in prange(1,n):\n",
    "        sum = sum + np.absolute(signal1[m+i*k]-signal1[m+(i-1)*k])\n",
    "    Lmq = (sum * norm) / k\n",
    "    return Lmq\n",
    "@njit\n",
    "def Lq_k(signal, k, fs):\n",
    "    #calc_L_series = np.frompyfunc(lambda m: Lm_q(signal, m, k, fs), 1, 1)\n",
    "    calc_L_series = np.zeros(k)\n",
    "    for m in prange(1,k+1):\n",
    "        calc_L_series[m-1] = Lm_q(signal,m,k,fs)\n",
    "    L_average = np.mean(calc_L_series)\n",
    "    return L_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###THE true challenge : \n",
    "@njit\n",
    "def TSD_mean_calculator_fast(signal2,segment_length,fs):\n",
    "    Ds = np.zeros(int(len(signal2)-segment_length))\n",
    "    for w in prange(1,int(len(signal2)-segment_length)):\n",
    "        sig_true = signal2[int((w - 1)): int((w)+segment_length)]\n",
    "        L1 = Lq_k(sig_true, 1, fs)\n",
    "        L2 = Lq_k(sig_true,2,fs)\n",
    "        Ds[w] = (np.log(L1) - np.log(L2)) / (np.log(2))\n",
    "    return np.mean(Ds), np.std(Ds)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3661486613441125\n",
      "1.3666953394799033\n"
     ]
    }
   ],
   "source": [
    "##Test :\n",
    "val,_ = TSD_mean_calculator_fast(sig,2500,fs)\n",
    "print(val)\n",
    "\n",
    "##Comparison : \n",
    "def TSD_mean_calculator(signal,segment_length,fs,k=1):\n",
    "\n",
    "    X = np.c_[[signal[int((w - 1)): int((w)+segment_length)] for w in range(1, int(len(signal)-segment_length),k)]]\n",
    "    L1 = np.array([Lq_k(X[i, :], 1, fs) for i in range(X.shape[0])])\n",
    "    L2 = np.array([Lq_k(X[i, :], 2,fs) for i in range(X.shape[0])])\n",
    "    Ds = (np.log(L1) - np.log(L2)) / (np.log(2))\n",
    "    return np.mean(Ds), np.std(Ds)\n",
    "val_c,_ = TSD_mean_calculator(sig,2500,fs)\n",
    "print(val_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###THE UTILMATE TEST NIGH!!! THIS TEST, IF SUCCEEDED, WILL RULE THEM ALLLLLLLLL MWAHAHAHHAHAHAHAHAHAHHAHHAHAHAHAHA :\n",
    "\n",
    "def is_segment_flatline(sig):\n",
    "    cond = np.where(np.diff(sig.copy(),1) != 0.0, False, True)\n",
    "    if len(cond[cond == True]) < 0.70 * len(sig):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def TSD_index_fast(dico_signal, name_lead, fs):\n",
    "\n",
    "    ###Index Creation :TSD\n",
    "    ###The label will be as follow : mean(TSD) < 1.25 = Acceptable;mean(SDR of all lead) >1.25 = Unacceptable\n",
    "    ##For each lead, we will return a more precise classification based on the folloying rules:\n",
    "    ## TSD<1.25 = Good quality ; 1.25<TSD<1.40 = Medium quality; TSD>1.4 = Bad quality\n",
    "    # dico_seg = Interval_calculator(dico_signal,name_lead,fs,t0)\n",
    "    dico_D = {}\n",
    "    D_arr = np.array([])\n",
    "    dic_segment = Interval_calculator_all(dico_signal,name_lead,fs)\n",
    "    #dic_segment = 2500\n",
    "    for i in name_lead:\n",
    "        if is_segment_flatline(dico_signal[i]):\n",
    "            dico_D[i] = (2,dico_signal[i])\n",
    "            D_arr = np.append(D_arr,2)\n",
    "        else :\n",
    "            Dv, _ = TSD_mean_calculator_fast(dico_signal[i],dic_segment[i],fs)\n",
    "            dico_D[i] = (Dv, dico_signal[i])\n",
    "            D_arr = np.append(D_arr, Dv)\n",
    "    return dico_D, np.mean(D_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'I': (1.3661558656318777, array([-0.02 , -0.015, -0.01 , ...,  0.42 ,  0.47 ,  0.5  ], dtype=float32)), b'II': (1.3348961700832476, array([-0.06, -0.06, -0.07, ...,  0.28,  0.4 ,  0.54], dtype=float32)), b'III': (1.4068697738421014, array([-0.04 , -0.045, -0.06 , ..., -0.14 , -0.07 ,  0.04 ], dtype=float32)), b'aVR': (1.371269766024023, array([ 0.04 ,  0.035,  0.04 , ..., -0.35 , -0.435, -0.52 ], dtype=float32)), b'aVF': (1.4059469093817505, array([-0.05 , -0.05 , -0.065, ...,  0.07 ,  0.165,  0.29 ], dtype=float32)), b'aVL': (1.4283963160087998, array([0.01 , 0.015, 0.025, ..., 0.28 , 0.27 , 0.23 ], dtype=float32)), b'V1': (1.4604248968016966, array([ 0.06 ,  0.07 ,  0.075, ..., -0.055, -0.12 , -0.2  ], dtype=float32)), b'V2': (1.1890368473767963, array([-0.06 , -0.07 , -0.07 , ...,  1.075,  1.035,  0.94 ], dtype=float32)), b'V3': (1.2198807830096552, array([-0.04 , -0.05 , -0.055, ...,  1.06 ,  1.08 ,  1.06 ], dtype=float32)), b'V4': (1.2076823561799914, array([-0.04 , -0.05 , -0.055, ...,  1.115,  1.19 ,  1.22 ], dtype=float32)), b'V5': (1.255352948386178, array([-0.06, -0.06, -0.06, ...,  1.05,  1.15,  1.22], dtype=float32)), b'V6': (1.2722053327038374, array([-0.06, -0.06, -0.06, ...,  0.89,  1.  ,  1.1 ], dtype=float32))}\n"
     ]
    }
   ],
   "source": [
    "## AND NOW THE TEST:\n",
    "\n",
    "dico_d,val = TSD_index_fast(dico_ECG,ECG_lead,fs)\n",
    "print(dico_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
