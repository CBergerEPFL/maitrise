{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm import make_reader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import os\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "from shared_utils.utils_data import format_data_to_xarray_2020,format_data_to_xarray\n",
    "\n",
    "path_formatted_glasgow = \"/workspaces/maitrise/data/20220902_data_physio_formatted_merged/merged/dataParquet\"\n",
    "path_petastorm = f\"file:///{path_formatted_glasgow}\"\n",
    "\n",
    "path_formated_cinc2011= \"/workspaces/maitrise/data/20221006_physio_quality/set-a/dataParquet\"\n",
    "path_petastorm_cinc2011 = f\"file:///{path_formated_cinc2011}\"\n",
    "\n",
    "save_path = \"/workspaces/maitrise/results\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (save_path is not None) and (not os.path.exists(save_path)):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "\n",
    "##What we need : \n",
    "##1) Patient ID\n",
    "##2) Lead names(save as indexes)\n",
    "##3) Pathology (repeated on eahc lead)\n",
    "##4) ECG signal\n",
    "i_stop = 50\n",
    "ECG_signals = torch.zeros((50,5000,12))\n",
    "Leads_index  = torch.zeros((50,12))\n",
    "SQA_score = torch.zeros((50,12))\n",
    "Pathologys = torch.zeros((50))\n",
    "with make_reader(path_petastorm) as reader:\n",
    "    for idx, sample in enumerate(reader):\n",
    "        if idx == 0:\n",
    "            lead_names = sample.signal_names.astype(str)\n",
    "        if len(sample.signal[:,0])!=5000:\n",
    "            continue\n",
    "        \n",
    "        if idx == i_stop :\n",
    "            break\n",
    "        else : \n",
    "\n",
    "            Leads_index[idx,:] = torch.tensor(list(range(12)))\n",
    "            ECG_signals[idx,:,:] = torch.tensor(sample.signal[:,:])\n",
    "            if 0 in sample.score_classes:\n",
    "                Pathologys[idx] = torch.tensor(int(sample.diagnostics[0]))\n",
    "            else : \n",
    "                Pathologys[idx] = torch.tensor(int(sample.diagnostics[np.where(sample.diagnostics == sample.score_classes[0])]))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convention :\n",
    "\n",
    "For the lead names, we will use the following convention : \n",
    "\n",
    "|I|II|III|aVR|aVL|aVF|V1|V2|V3|V4|V5|V6|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|0|1|2|3|4|5|6|7|8|9|10|11|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Generator and Discriminator\n",
    "\n",
    "Class for Discrimnator and Generator. *THIS IS TEMPORARY*. It will be displaced in another git later. It willbe only used here to check if the Conditional GAN do what it must do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_channels,features = 32,n_lead = 12,n_classes = 13):\n",
    "        ##in_channels = 1\n",
    "        super(Discriminator,self).__init__()\n",
    "        ##Input : Batch_size*1*len_seq\n",
    "        #self.Embedding_path = nn.Embedding(n_classes,len_seq)\n",
    "        #self.Embedding_lead = nn.Embedding(n_lead,len_seq)\n",
    "        self.model = nn.Sequential(\n",
    "            self._Block(in_channels,features,1),\n",
    "            self._Block(features,features,2),\n",
    "            self._Block(features,features*2,1),\n",
    "            self._Block(features*2,features*2,2),\n",
    "            self._Block(features*2,features*4,1),\n",
    "            self._Block(features*4,features*4,2),\n",
    "            self._Block(features*4,features*8,1),\n",
    "            self._Block(features*8,features*8,2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.out1 = nn.Sequential(nn.Linear(features*8,1),nn.Sigmoid())\n",
    "        #self.out2 = nn.Sequential(nn.Linear(features,n_classes),nn.Softmax())\n",
    "\n",
    "    def _Block(self,in_channels,out_channels,stride,kernel_size=3,padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels,out_channels,kernel_size,stride,padding,padding_mode = \"zeros\",bias = False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "    def forward(self,x,lab_signal=0,lab_path=0):\n",
    "        #d_in = torch.cat([x,self.Embedding_lead(lab_signal),self.Embedding_path(lab_path)],0)\n",
    "        o = self.model(x)\n",
    "        validity = self.out1(o)\n",
    "        #path_class = self.out2(o)\n",
    "        return validity\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim,in_channels,features = 32,len_seq = 5000):\n",
    "        super(Generator,self).__init__()\n",
    "        self.features = features\n",
    "        self.z_dim = z_dim\n",
    "        self.len_seq = len_seq\n",
    "        self.down_1 = self._downsampling_block(self.features,self.features,2)\n",
    "        self.skip_attention_1 = self._downsampling_block(self.features,self.features,1,kernel_size=2,dillation_rate=2)\n",
    "        self.down_2 = self._downsampling_block(self.features*2,self.features*2,2)\n",
    "        self.skip_attention_2 = self._downsampling_block(self.features*2,self.features*2,1,kernel_size=2,dillation_rate=2)\n",
    "        self.down_3 = self._downsampling_block(self.features*4,self.features*4,2)\n",
    "        self.skip_attention_3 = self._downsampling_block(self.features*4,self.features*4,1,kernel_size=2,dillation_rate=2)\n",
    "        self.up_3 = self._upsampling_block(self.features*4,self.features*4,2)\n",
    "        self.up_2 = self._upsampling_block(self.features*4,self.features*2,2)\n",
    "        self.up_1 = self._upsampling_block(self.features*2,self.features,2)\n",
    "        self.final = nn.Sequential(nn.Conv1d(self.features,in_channels,kernel_size = 3,stride = 1,padding = 1,padding_mode = \"zeros\"),nn.Sigmoid())\n",
    "\n",
    "    def _downsampling_block(self,in_channels,out_channels,stride,kernel_size=3,padding=1,dillation_rate = 1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels,out_channels,kernel_size,stride,padding,padding_mode = \"zeros\",bias = False,dilation = dillation_rate),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "\n",
    "    def _upsampling_block(self,in_channels,out_channels,stride,kernel_size=3,padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels,out_channels,kernel_size,stride,padding,padding_mode = \"zeros\",bias = False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "\n",
    "    def _novel_residual_block(self,x,out_channels,stride,kernel_size=3,padding=1):\n",
    "        fe_add = nn.Conv1d(x.size(1),out_channels,kernel_size,stride,padding,padding_mode=\"zeros\")(x)\n",
    "        fe = nn.BatchNorm1d(out_channels)(fe_add)\n",
    "        fe = nn.LeakyReLU(0.25)(fe)\n",
    "        fe = torch.add(fe,fe_add)\n",
    "        return fe\n",
    "\n",
    "    def forward(self,x):\n",
    "        gen = self._novel_residual_block(x,self.features,1)\n",
    "        skip_1 = self.skip_attention_1(gen)\n",
    "        gen = self.down_1(gen)\n",
    "        gen = self._novel_residual_block(gen,self.features*2,1)\n",
    "        skip_2 = self.skip_attention_2(gen)\n",
    "        gen = self.down_2(gen)\n",
    "        gen = self._novel_residual_block(gen,self.features*4,1)\n",
    "        skip_3 = self.skip_attention_3(gen)\n",
    "        gen = self.down_3(gen)\n",
    "        gen = self.up_3(gen)\n",
    "        gen = torch.add(gen,skip_3)\n",
    "        gen = self.up_2(gen)\n",
    "        gen = torch.add(gen,skip_2)\n",
    "        gen = self.up_1(gen)\n",
    "        gen = torch.add(gen,skip_1)\n",
    "        ECG_reconstruct = self.final(gen)\n",
    "        return ECG_reconstruct\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv1d,nn.ConvTranspose1d,nn.BatchNorm1d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "\n",
    "def test():\n",
    "    N,in_channels,HW= 8,5000,1\n",
    "    z_dim = 1000\n",
    "    x = torch.rand((N,in_channels,HW))\n",
    "    disc = Discriminator(in_channels,features= 8)\n",
    "    assert disc(x).shape == (N,1)\n",
    "    print(\"Discriminator OK\")\n",
    "\n",
    "    ##Generator :\n",
    "    gen = Generator(z_dim,in_channels,8)\n",
    "    z = torch.rand((N,z_dim,1))\n",
    "    assert gen(z).shape== (N,in_channels,1)\n",
    "    print(\"Generator OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The dataset \n",
    "\n",
    "class DatasetECGReconstruction(Dataset):\n",
    "    def __init__(self,signals,leads,pathology):#quality_scores):\n",
    "        self.ECGs = signals\n",
    "        self.leads = leads\n",
    "        self.path = pathology\n",
    "        #self.SQA = quality_scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        Signal = self.ECGs[index]\n",
    "        path_patient = self.path[index]\n",
    "        #score_leads = self.SQA[index]\n",
    "        leads = self.leads\n",
    "        return {\"ECG recording\":Signal,\"pathology\":path_patient,\"Leads index\":leads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ECG = DatasetECGReconstruction(ECG_signals,Leads_index,Pathologys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_ECG = DataLoader(d_ECG,batch_size=8,shuffle = True)\n",
    "print(DL_ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index,batch) in enumerate(DL_ECG):\n",
    "    print(f\"For index {index},we have the follwoing batch : \")\n",
    "    print(batch[\"pathology\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
